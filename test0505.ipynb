{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "Node = namedtuple('Node', ['t', 'm'])  # Store the node in binomial tree  [at time t move up m times]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "class BinomialTree:\n",
    "    def __init__(self, s0 = 100, K = 100, r = 0.02, sigma = 0.1, maturity = 1, n = 10, D = 1):\n",
    "        self.s0 = s0\n",
    "        self.K = K  # ATM strike price\n",
    "        self.r = r  # riskless interest rate\n",
    "        self.sigma = sigma  # stocks vol\n",
    "        self.T = maturity  # option's maturity\n",
    "        self.n = n  # total steps of Tree\n",
    "        self.delta_t = self.T / self.n\n",
    "\n",
    "\n",
    "        ## Calculate key elements in Tree generalization\n",
    "        self.u = np.exp(self.sigma * np.sqrt(self.delta_t))  # Move up steps\n",
    "        self.d = 1 / self.u\n",
    "        self.R = np.exp(self.r * self.delta_t)  # Discount ratio of each step\n",
    "        self.p = (self.R - self.d) / (self.u - self.d)  # Prob of moving up\n",
    "\n",
    "        ## Self state\n",
    "        self.node = None  # current state: Node(t, m)\n",
    "        self.hedging_pos = 0  # Started hedging position\n",
    "        self.D = D  # Holding position of derivatives\n",
    "\n",
    "        ## Env state\n",
    "        self.grid = None  # Store the tree's Node:  {time t: [Node(t, m1), Node(t, m2) ...]}\n",
    "        self.observation_space = None  # Dict: {Node(t, m): (stock_price, option_price, {'exercise': bool})}\n",
    "        self.action_space = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "        self.state_dim = 1\n",
    "        self.action_dim = len(self.action_space)\n",
    "\n",
    "\n",
    "    def get_price(self, node):  # Return the stock price at Node(i,j)\n",
    "        return self.s0 * np.power(self.u, node.m)\n",
    "\n",
    "    def _generate(self):\n",
    "        grid = {}\n",
    "        for t in range(self.n + 1):\n",
    "            grid[t] = []\n",
    "            for i in range(-t, t+1, 2):\n",
    "                grid[t].append(Node(t, i))  # Node at time t\n",
    "        self.grid = grid\n",
    "\n",
    "        # return grid\n",
    "\n",
    "    def _back_propagation(self):\n",
    "        assert self.grid, 'Grid must be generated first!'\n",
    "        obs_space = {}  # Restore data at each node {Node : (stock price, option value)\n",
    "\n",
    "        for t in range(self.n, 0 - 1, -1):\n",
    "\n",
    "            if t == self.n:  # Value at maturity\n",
    "                for node in self.grid[t]:\n",
    "                    stock_p = self.get_price(node)\n",
    "                    opt_v = max(self.K - stock_p, 0)  # Put option value at maturity\n",
    "                    obs_space[node] = (stock_p, opt_v, {'exercise': True})  # Restore current price & value\n",
    "            else:  # Value before maturity\n",
    "                for node in self.grid[t]:\n",
    "                    m = node.m\n",
    "                    stock_p = self.get_price(node)\n",
    "                    opt_exe_v = max(self.K - stock_p, 0)\n",
    "                    opt_continue_v = (self.p * (obs_space[Node(t + 1, m + 1)][1]) + (1 - self.p) * (obs_space[Node(t + 1, m - 1)][1])) / self.R\n",
    "\n",
    "                    if opt_exe_v > opt_continue_v:\n",
    "                        opt_v = opt_exe_v\n",
    "                        obs_space[node] = (stock_p, opt_v, {'exercise': True})\n",
    "                    else:\n",
    "                        opt_v = opt_continue_v\n",
    "                        obs_space[node] = (stock_p, opt_v, {'exercise': False})\n",
    "        self.observation_space = obs_space\n",
    "        # return obs_space\n",
    "\n",
    "    def fit(self):  # Initialized the environment\n",
    "        self._generate()\n",
    "        self._back_propagation()\n",
    "        print('Environment initialization completed!')\n",
    "\n",
    "\n",
    "    def get_holding_value(self, stock_p, opt_v, hedge_pos):\n",
    "        return hedge_pos * stock_p - opt_v * self.D\n",
    "\n",
    "    def reset(self):\n",
    "        assert self.observation_space, 'Env must be fited first!'\n",
    "        self.node = Node(0, 0)  # Initial state\n",
    "        self.hedging_pos = 0\n",
    "        stock_p, opt_v, _ = self.observation_space[self.node]\n",
    "\n",
    "        # holding_value = self.get_holding_value(stock_p, opt_v, self.hedging_pos)\n",
    "        # state = (stock_p, holding_value)\n",
    "        state = (stock_p, opt_v)\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n",
    "    def reward(self, stock_p, opt_v, hedging_pos_new, w = 0.001):\n",
    "        \"\"\"\n",
    "        Considering the transaction cost in each step and the final payoff\n",
    "        :param stock_p: current stock price\n",
    "        :param opt_v: current option value\n",
    "        :param hedging_pos_new:  current action\n",
    "        :param w: trading cost coefficient\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        trading_cost = - stock_p * (hedging_pos_new - self.hedging_pos) * w  # Negative trading reward\n",
    "        hedging_diff = - np.square(self.get_holding_value(stock_p, opt_v, hedging_pos_new))  # Negative hedging diff\n",
    "        return hedging_diff +  trading_cost\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: hedging amount (continuous num)\n",
    "        :return: stock_price, reward, done, {\"exercise\": bool}\n",
    "        \"\"\"\n",
    "        assert self.node, 'Must reset the environment first'\n",
    "        assert action in self.action_space, \"Input action is not in action space\"\n",
    "        t, m = self.node\n",
    "\n",
    "        if t == self.n:\n",
    "            stock_p, opt_v, exercise = self.observation_space[self.node]\n",
    "            # holding_val = self.get_holding_value(stock_p, opt_v, action)\n",
    "            # state = (stock_p, holding_val)\n",
    "            state = (stock_p, opt_v)\n",
    "\n",
    "            done = True\n",
    "            reward = self.reward(stock_p, opt_v, action)\n",
    "\n",
    "            return state, reward, done, {}\n",
    "\n",
    "        else:\n",
    "            if np.random.binomial(1, self.p):  # Binomial transition prob\n",
    "                node = Node(t + 1, m + 1)\n",
    "            else:\n",
    "                node = Node(t + 1, m - 1)\n",
    "\n",
    "            stock_p, opt_v, exercise = self.observation_space[node]\n",
    "\n",
    "            # holding_val = self.get_holding_value(stock_p, opt_v, action)\n",
    "            # state = (stock_p, holding_val)\n",
    "            state = (stock_p, opt_v)\n",
    "\n",
    "            done = exercise['exercise']\n",
    "            reward = self.reward(stock_p, opt_v, action)\n",
    "\n",
    "            ## Update self state\n",
    "            self.hedging_pos = action\n",
    "            self.node = node\n",
    "\n",
    "            return np.array(state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "g = BinomialTree()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialization completed!\n"
     ]
    }
   ],
   "source": [
    "g.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([100.       ,   3.1800015], dtype=float32)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node(t=0, m=0)\n",
      "0\n",
      "(array([103.21281  ,   1.8562732], dtype=float32), -71.6666770122375, False, {})\n",
      "Node(t=1, m=1)\n",
      "0.1\n",
      "------------------\n",
      "(array([106.52884   ,   0.91756433], dtype=float32), -94.77644721051321, False, {})\n",
      "Node(t=2, m=2)\n",
      "0.1\n",
      "------------------\n",
      "(array([103.21281  ,   1.5499569], dtype=float32), -76.9361234785206, False, {})\n",
      "Node(t=3, m=1)\n",
      "0.1\n",
      "------------------\n",
      "(array([100.       ,   2.5517154], dtype=float32), -55.476945068875246, False, {})\n",
      "Node(t=4, m=0)\n",
      "0.1\n",
      "------------------\n",
      "(array([96.8872   ,  4.0684943], dtype=float32), -31.586935764174466, False, {})\n",
      "Node(t=5, m=-1)\n",
      "0.1\n",
      "------------------\n",
      "(array([93.87129 ,  6.228697], dtype=float32), -9.975697752194499, False, {})\n",
      "Node(t=6, m=-2)\n",
      "0.1\n",
      "------------------\n",
      "(array([96.8872  ,  3.686346], dtype=float32), -36.02849231495374, False, {})\n",
      "Node(t=7, m=-1)\n",
      "0.1\n",
      "------------------\n",
      "(array([93.87129 ,  6.128706], dtype=float32), -10.617324069100846, True, {})\n",
      "Node(t=8, m=-2)\n",
      "0.1\n",
      "------------------\n",
      "(array([90.949265,  9.050732], dtype=float32), -0.0019531770709955974, True, {})\n",
      "Node(t=9, m=-3)\n",
      "0.1\n",
      "------------------\n",
      "(array([93.87129 ,  6.128706], dtype=float32), -10.617324069100846, True, {})\n",
      "Node(t=10, m=-2)\n",
      "0.1\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "g.reset()\n",
    "print(g.node)\n",
    "print(g.hedging_pos)\n",
    "for i in range(10):\n",
    "    print(g.step(0.1))\n",
    "    print(g.node)\n",
    "    print(g.hedging_pos)\n",
    "    print('------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vanilla Policy Gradient (REINFORCE)\n",
    "using pytorch to build simple reinforce algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, hidden_size, learning_rate = 3e-4):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch, state_dim)  batch:一次trajectory中的steps\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim = 1)\n",
    "        return x\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = state.unsquzze(0).to(device)\n",
    "\n",
    "        probs = self.forward(state).cpu()\n",
    "        # highest_prob = np.random.choice(self.action_dim,  p=np.squeeze(prob.detach().numpy()))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        # log_prob = torch.log(prob.squeeze(0)[highest_prob])\n",
    "\n",
    "        return action.item() / 10, m.log_prob(action)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class AgentVPG:\n",
    "    GAMMA = 0.9\n",
    "    max_episodes = 2000\n",
    "\n",
    "    def __init__(self, env, policy_net):\n",
    "        self.env = env\n",
    "        self.policy_net = policy_net\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def discounted_future_reward(rewards: list):\n",
    "        discounted_r = [rewards[-1]]\n",
    "\n",
    "        for r in rewards[-2::-1]:\n",
    "            rr = AgentVPG.GAMMA * discounted_r[-1]\n",
    "            Gt = r + rr\n",
    "            discounted_r.append(Gt)\n",
    "        discounted_r = discounted_r[::-1]\n",
    "        return discounted_r\n",
    "\n",
    "    def update_policy(self, rewards, log_probs):\n",
    "        discounted_rewards = self.discounted_future_reward(rewards)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "        policy_grads = []\n",
    "        for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
    "            policy_grads.append(-log_prob*Gt)\n",
    "\n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        policy_grad = torch.stack(policy_grads).sum()\n",
    "        policy_grad.backward()\n",
    "        self.policy_net.optimizer.step()\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        # num_steps = []\n",
    "        # avg_num_steps = []\n",
    "        all_rewards = []\n",
    "        mean_rewards = []\n",
    "\n",
    "        for episode in range(AgentVPG.max_episodes):\n",
    "            state = self.env.reset()\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            for step in range(15):  # total steps is 10\n",
    "\n",
    "                action, log_prob = self.policy_net.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    # 完成一次 episode/rollout，得到一次完整的 trajectory\n",
    "                    self.update_policy(rewards, log_probs)\n",
    "                    # num_steps.append(step)\n",
    "                    # avg_num_steps.append(np.mean(num_steps[-3:]))\n",
    "\n",
    "                    all_rewards.append(sum(rewards))\n",
    "                    mean_rewards.append(np.mean(rewards))\n",
    "                    if episode % 100 == 0:\n",
    "                        print(f'episode: {episode}, total reward: {sum(rewards)}, mean_reward: {np.mean(rewards)}, length: {step}')\n",
    "                    break\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "        plt.plot(all_rewards)\n",
    "        plt.plot(mean_rewards)\n",
    "        plt.legend(['all_rewards', 'mean_rewards'])\n",
    "        plt.xlabel('episode')\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialization completed!\n"
     ]
    }
   ],
   "source": [
    "binomial_env = BinomialTree()\n",
    "binomial_env.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "VPG_policy = PolicyNetwork(binomial_env.state_dim, binomial_env.action_dim, 16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "agent = AgentVPG(binomial_env, VPG_policy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'unsquzze'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36mAgentVPG.fit\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     45\u001B[0m rewards \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m15\u001B[39m):  \u001B[38;5;66;03m# total steps is 10\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m     action, log_prob \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_net\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoose_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m     next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     51\u001B[0m     log_probs\u001B[38;5;241m.\u001B[39mappend(log_prob)\n",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36mPolicyNetwork.choose_action\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchoose_action\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[0;32m     20\u001B[0m     state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(state)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m---> 21\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsquzze\u001B[49m(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     23\u001B[0m     probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(state)\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# highest_prob = np.random.choice(self.action_dim,  p=np.squeeze(prob.detach().numpy()))\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'unsquzze'"
     ]
    }
   ],
   "source": [
    "agent.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "ss = binomial_env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([100.       ,   3.1800015], dtype=float32)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "tt = torch.from_numpy(ss).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "tt = tt.unsqueeze(0).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# torch.from_numpy(state).float().unsquzze(0).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[100.0000,   3.1800]], device='cuda:0')"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size, learning_rate = 0.9):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)  # 这个dim是 0 or 1？\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item() / 10, m.log_prob(action)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# Create the env\n",
    "# env = gym.make(env_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "0.049145546"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.reset()[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "pp = Policy(1, 8, 16).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.0, tensor([-1.9138], grad_fn=<SqueezeBackward1>))"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.act(np.array([1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}